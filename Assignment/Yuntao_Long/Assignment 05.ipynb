{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 01 – Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本块应该可以不运行，给看一下有就行了。反正就是用之前的函数下载两个txt文件（原来程序直接保存为地址上的文件名，没有用剧名保存）\n",
    "\n",
    "# copied from 04\n",
    "\n",
    "import urllib\n",
    "import logging\n",
    "\n",
    "# defined in Task 05\n",
    "def init_log(file_name, file_mode, level, format, date_format):\n",
    "    logging.basicConfig(filename=file_name,\n",
    "                        filemode=file_mode,\n",
    "                        level=level,\n",
    "                        format=format,\n",
    "                        datefmt=date_format)\n",
    "\n",
    "def download_file(url, path):\n",
    "    # check whether the urls points to a .txt file by verifying its suffix\n",
    "    if url.endswith(\".txt\"):\n",
    "        try:\n",
    "            # open the link\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'} \n",
    "            req = urllib.request.Request(url=url,headers=headers)\n",
    "            f = urllib.request.urlopen(req)\n",
    "            \n",
    "            # get the name of txt file:   https://www.google.com/s/123.txt ---> 123.txt\n",
    "            url=url[::-1] # reverse the url:  txt.321/s/moc.elgoog.www//:sttph\n",
    "            x=url.index(\"/\")  # the index of the first \"/\"\n",
    "            filename=url[:x]  # cut the string and get the left slice:  txt.321\n",
    "            filename=filename[::-1]  # reverse :123.txt\n",
    "            \n",
    "            # read data and write it into a file\n",
    "            data = f.read()\n",
    "            with open(path+filename, 'wb') as f2:\n",
    "                f2.write(data)\n",
    "                \n",
    "        # exception: invalid link\n",
    "        except urllib.error.URLError:\n",
    "            print(f\"Error: cannot access the link {url}\")\n",
    "        # exception: target path invalid\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: the given path does not exist\")\n",
    "            \n",
    "    # does not point to a txt file\n",
    "    else:\n",
    "        logging.error(\"No text file found at given URL, download aborted!\")  # redefine the error message\n",
    "        \n",
    "\n",
    "init_log(\"task05.log\",\"a\",logging.DEBUG,'%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\"%Y-%m-%d %H:%M:%S\")\n",
    "path=\"./\"  # the current folder\n",
    "\n",
    "\n",
    "# 0. download the plain text versions of Shakespeare’s play Macbeth and Bacon’s New Atlantis.\n",
    "\n",
    "\n",
    "url1=\"https://ia802707.us.archive.org/1/items/macbeth02264gut/0ws3410.txt\"\n",
    "url2=\"https://ia801309.us.archive.org/24/items/newatlantis02434gut/nwatl10.txt\"\n",
    "\n",
    "download_file(url1, path)\n",
    "download_file(url2, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get_speaker_text()- returns only the text spoken by the characters in the plays and removes all other text\n",
    "\n",
    "TITLES=[\"THE NEW ATLANTIS\",\"The Tragedie of Macbeth\"]\n",
    "\n",
    "def get_speaker_text(text):\n",
    "    # Atlantis\n",
    "    if \"THE NEW ATLANTIS\" in text:\n",
    "        # extract only the main body\n",
    "        i=text.index(\"THE NEW ATLANTIS\")\n",
    "        l=len(\"THE NEW ATLANTIS\")\n",
    "        text=text[i+l:]\n",
    "        j=text.index(\"[The rest was not perfected.]\")\n",
    "        text=text[:j]\n",
    "    # Macbeth\n",
    "    else:\n",
    "        i=text.index(\"The Tragedie of Macbeth\")\n",
    "        l=len(\"The Tragedie of Macbeth\")\n",
    "        text=text[i+l:]\n",
    "        lines=text.split(\"\\n\") # separate lines\n",
    "        lines=[line.strip() for line in lines]  # remove extra space and \\n\n",
    "        text=\"\"\n",
    "        for line in lines:\n",
    "            # ignore scene instructions\n",
    "            if \"Scena\" in line or \"Enter\" in line or \"Exeunt\" in line:  \n",
    "                continue\n",
    "            # remove characters' names (a \".\" after the character:  king. his words)\n",
    "            if \".\" in line:\n",
    "                i=line.index(\".\")\n",
    "                line=line[i+1:]  # ignore the substring before .\n",
    "            text=text+\" \"+line\n",
    "    return text\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. normalize_text()\n",
    "\n",
    "\n",
    "def normalize_text(text,correct=False):\n",
    "    import string\n",
    "    import utils_ocr\n",
    "    \n",
    "    text=text.lower()  #converts all text to lower case\n",
    "    \n",
    "    # if necessary: ocr error correction\n",
    "    if correct:\n",
    "        text=utils_ocr.correct_ocr_errors(text)\n",
    "    \n",
    "    # removes all punctuation from the texts\n",
    "    for p in string.punctuation:\n",
    "        text=text.replace(p,\" \")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 remove_stopwords()\n",
    "def remove_stopwords(text):\n",
    "    text=text.replace(\"\\n\",\" \")  # merge all text into one line\n",
    "    \n",
    "    # read stopwords from the given file\n",
    "    with open(\"eng_stop_words.txt\",\"r\") as f:\n",
    "        stopwords=f.readlines()\n",
    "    \n",
    "    stopwords=[x.strip() for x in stopwords] # remove extra space or \"\\n\" for each word\n",
    "    # remove each stopword\n",
    "    for word in stopwords:\n",
    "        text=text.replace(\" \"+word+\" \",\" \")  # add two spaces beside each stopword to ensure the matched substring is a word\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. tokenize_text() – splits the cleaned text into words\n",
    "def tokenize_text(text):\n",
    "    words=text.split()  # divide the long text string into words by space\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "ftest=\"Macbeth.txt\"\n",
    "with open(ftest,\"r\") as f:\n",
    "    testtext=f.read()\n",
    "testtext=get_speaker_text(testtext)\n",
    "testtext=normalize_text(testtext)\n",
    "testtext=remove_stopwords(testtext)\n",
    "testwords=tokenize_text(testtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 02 – Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "# define a class to process the txt file\n",
    "class TextWords:\n",
    "    def __init__(self,filename,ocr=False):\n",
    "        # 0. get the text from file\n",
    "        with open(filename,\"r\") as f:\n",
    "            self.text=f.read()\n",
    "        # pre-process\n",
    "        self.text=get_speaker_text(self.text)      #1\n",
    "        self.text=normalize_text(self.text,ocr)    #2\n",
    "        self.text=remove_stopwords(self.text)      #3\n",
    "        self.words=tokenize_text(self.text)        #4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1=\"NewAtlantis.txt\"\n",
    "filename2=\"Macbeth.txt\"\n",
    "\n",
    "newatlantis=TextWords(filename1)\n",
    "macbeth=TextWords(filename2,True)  # Macbeth needs to correct ocr errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 \n",
    "words1=set(newatlantis.words)\n",
    "words2=set(macbeth.words)\n",
    "common=words1&words2    # the intersections of two sets contains the common elements shared by two sets\n",
    "\n",
    "fre=[]  # the frequency list of common words\n",
    "for term in common:\n",
    "    cnt1=newatlantis.words.count(term)  # frequency_doc1\n",
    "    cnt2=macbeth.words.count(term)  # frequency_doc2\n",
    "    fre.append([term,cnt1,cnt2,cnt1+cnt2])  # [term , frequency_doc1 , frequency_doc2 , sum_of_frequencies]\n",
    "\n",
    "fre.sort(key=lambda x:x[3],reverse=True)   # sort the list by sum of the frequencies in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "import csv\n",
    "\n",
    "csvname=\"fre.csv\"\n",
    "\n",
    "with open(csvname,\"w\",newline=\"\") as f:\n",
    "    wr=csv.writer(f)\n",
    "    wr.writerows(fre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
